{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2xDeDnsXsccAy3hv4Vnwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SridharSola/Knowledge-Distillation-FER/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZrpcRjj5fwZ"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer): \n",
        "  for param_group in optimizer.param_groups: \n",
        "      param_group[\"lr\"] /= 10.\n",
        "\n",
        "def accuracy(out, labels):\n",
        "    _,pred = torch.max(out, dim=1)\n",
        "    return torch.sum(pred==labels).item()\n",
        "\n",
        "def training(teacher, student, classifier, train_set, test_set, learning_rate, epochs, lamda, log, model_save, class_save):\n",
        "  if torch.cuda.is_available():\n",
        "    teacher.cuda()\n",
        "    student.cuda()\n",
        "    classifier.cuda()\n",
        "  teacher.eval()\n",
        "  student.train()\n",
        "  classifier.train()\n",
        "  optimizer = torch.optim.SGD([{'params':student.parameters(), 'lr' : learning_rate, 'momentum': 0.9, 'weight_decay': 0.00005},\n",
        "                               {'params' : classifier.parameters(), 'lr' : learning_rate, 'momentum' : 0.9, 'weight_decay': 0.00005}\n",
        "                                ])\n",
        "  logfile = open(log, 'w')\n",
        "  n_epochs = epochs\n",
        "  print_every = 5\n",
        "  test_loss_min = np.Inf\n",
        "  test_loss_u = []\n",
        "  test_loss_m = []\n",
        "  \n",
        "  test_acc_emoU = []\n",
        "  test_acc_emoM =[]\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  Ubest_test_acc = 0.0\n",
        "  Mbest_test_acc = 0.0\n",
        "  total_step = len(train_set)\n",
        "  lrs = []\n",
        "  lrs.append(learning_rate)\n",
        "\n",
        "\n",
        "  #Losses:\n",
        "  Lce = nn.CrossEntropyLoss().cuda()\n",
        "  Lmse = nn.MSELoss().cuda()\n",
        "  ce = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "  print('\\nTraining starting:\\n', file = logfile)\n",
        "  print('\\nTraining starting:\\n')\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "      running_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      print(f'Epoch {epoch}\\n', file = logfile)\n",
        "      print(f'Epoch {epoch}\\n')\n",
        "      if epoch == 20 or epoch == 28 or epoch==36 or epoch == 50 or epoch == 60:     \n",
        "          #scheduler.step()\n",
        "          adjust_learning_rate(optimizer)\n",
        "          lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "          print(f'Updated lr: {lrs[-1]}\\n', file = logfile)\n",
        "          print(f'Updated lr: {lrs[-1]}\\n')\n",
        "      for batch_idx, (data_teach, data_stu, target_) in enumerate(train_set):\n",
        "        optimizer.zero_grad()\n",
        "        teacher_vector = teacher(data_teach)\n",
        "        student_vector = student(data_stu)\n",
        "        emo_probs = classifier(student_vector)\n",
        "\n",
        "        mse = Lmse(student_vector, teacher_vector)\n",
        "        ce = Lce(emo_probs, target_)\n",
        "        loss = ce + lamda*mse\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, emo_pred = torch.max(emo_probs, dim = 1)\n",
        "        correct += torch.sum(emo_pred==target_).item()\n",
        "        total += target_.size(0)\n",
        "        if (batch_idx) % 20 == 0:\n",
        "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch, n_epochs, batch_idx, total_step, loss.item()), file = logfile)\n",
        "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
        "      train_acc.append(100 * correct / total)\n",
        "      train_loss.append(running_loss/total_step)\n",
        "      print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}', file = logfile)\n",
        "      print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "      u_batch_loss = 0\n",
        "      m_batch_loss = 0\n",
        "      total_t=0\n",
        "      u_emo_correct_t=0\n",
        "      m_emo_correct_t=0\n",
        "      with torch.no_grad():\n",
        "          student.eval()\n",
        "          classifier.eval()\n",
        "          for udata_t, mdata_t, target_t in (test_set):\n",
        "              udata_t, mdata_t, target_t = udata_t.to(device), mdata_t.to(device), target_t.to(device)\n",
        "              \n",
        "              u_v = student(udata_t)\n",
        "              m_v = student(mdata_t)\n",
        "              \n",
        "              u_emo_probs_t = classifier(u_v)\n",
        "              m_emo_probs_t = classifier(m_v)\n",
        "\n",
        "              _,u_emo_pred_t = torch.max(u_emo_probs_t, dim=1)\n",
        "              #u_mask_pred_t = torch.round(u_mask_prob_t)\n",
        "              #u_mask_correct_t += torch.sum(u_mask_pred_t==target_u).item()\n",
        "              u_emo_correct_t += torch.sum(u_emo_pred_t==target_t).item()\n",
        "\n",
        "              _,m_emo_pred_t = torch.max(m_emo_probs_t, dim=1)\n",
        "              #m_mask_pred_t = torch.round(m_mask_prob_t)\n",
        "              #m_mask_correct_t += torch.sum(m_mask_pred_t== target_m).item()\n",
        "              m_emo_correct_t += torch.sum(m_emo_pred_t==target_t).item()\n",
        "\n",
        "              total_t += target_t.size(0)\n",
        "          #test_acc_maskU.append(100 * u_mask_correct_t/total_t)\n",
        "          test_acc_emoU.append(100 * u_emo_correct_t/total_t)\n",
        "          test_acc_emoM.append(100 * m_emo_correct_t/total_t)\n",
        "\n",
        "          network_learned = Ubest_test_acc < test_acc_emoU[-1] or Mbest_test_acc < test_acc_emoM[-1]\n",
        "          \n",
        "          print(f'Unmasked Test Accuracies: \\n FER: {(100 * u_emo_correct_t/total_t):.4f}', file = logfile)  \n",
        "          \n",
        "          print(f'Unmasked Test Accuracies: \\n FER: {(100 * u_emo_correct_t/total_t):.4f}')\n",
        "          \n",
        "         \n",
        "          print(f'Masked Test Accuracies: \\n FER: {(100 * m_emo_correct_t/total_t):.4f}', file = logfile)  \n",
        "          \n",
        "          print(f'Masekd Test Accuracies: \\n FER: {(100 * m_emo_correct_t/total_t):.4f}')\n",
        "        \n",
        "          if Ubest_test_acc < test_acc_emoU[-1]:\n",
        "            #Updating the best test_accuracy obtained\n",
        "            print(\"Best FER Test Accuracy Updated for unmasked set\", file = logfile)\n",
        "            print(\"Best FER Test Accuracy Updated for unmasked set\")\n",
        "            Ubest_test_acc = test_acc_emoU[-1]\n",
        "     \n",
        "          if Mbest_test_acc < test_acc_emoM[-1]:\n",
        "            #Updating the best test_accuracy obtained\n",
        "            print(\"Best FER Test Accuracy Updated for masked set\", file = logfile)\n",
        "            print(\"Best FER Test Accuracy Updated for masked set\")\n",
        "            Mbest_test_acc = test_acc_emoM[-1]\n",
        "          \n",
        "          if network_learned:\n",
        "              u_test_loss_min = u_batch_loss\n",
        "              torch.save(student.state_dict(), model_save)\n",
        "              torch.save(classifier.state_dict(), class_save)\n",
        "              print('Improvement-Detected for unmasked, save-model to drive', file = logfile)\n",
        "              print('Improvement-Detected for unmasked, save-model to drive')\n",
        "          \n",
        "\n",
        "      student.train()\n",
        "      classifier.train()\n",
        "\n",
        "  print(\"The best test accuracy obtained(Unmasked): \", Ubest_test_acc)\n",
        "  print(\"The best test accuracy obtained(Masked): \", Mbest_test_acc)\n",
        "  #End of training\n",
        "\n",
        "def training_teacher(net, cls, train_set, test_set, criterion, learning_rate, epochs, log, model_save, class_save):\n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "    cls.cuda()\n",
        "  criterion = criterion\n",
        "  optimizer = torch.optim.Adam([{'params':net.parameters(), 'lr' : learning_rate, 'momentum': 0.9, 'weight_decay': 0.00005},\n",
        "                               {'params' : cls.parameters(), 'lr' : learning_rate, 'momentum' : 0.9, 'weight_decay': 0.00005}\n",
        "                                ])\n",
        "  logfile = open(log, 'w')\n",
        "  n_epochs = epochs\n",
        "  print_every = 5\n",
        "  test_loss_min = np.Inf\n",
        "  test_loss = []\n",
        "  test_acc = []\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  best_test_acc = 0.0\n",
        "  total_step = len(train_set)\n",
        "  lrs = []\n",
        "  lrs.append(learning_rate)\n",
        "  print('\\nTraining starting:\\n', file = logfile)\n",
        "  print('\\nTraining starting:\\n')\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "      running_loss = 0.0\n",
        "      correct = 0\n",
        "      total=0\n",
        "      print(f'Epoch {epoch}\\n', file = logfile)\n",
        "      print(f'Epoch {epoch}\\n')\n",
        "      if epoch == 20 or epoch == 28 or epoch==36:     \n",
        "          #scheduler.step()\n",
        "          adjust_learning_rate(optimizer)\n",
        "          lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "          print(f'Updated lr: {lrs[-1]}\\n', file = logfile)\n",
        "          print(f'Updated lr: {lrs[-1]}\\n')\n",
        "      for batch_idx, (data_, target_) in enumerate(train_set):\n",
        "          data_, target_ = data_.to(device), target_.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputsV = net(data_)\n",
        "          outputs = cls(outputsV)\n",
        "          loss = criterion(outputs, target_)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          #out, probs = cls(outputs)\n",
        "          \n",
        "          running_loss += loss.item()\n",
        "          _,pred = torch.max(outputs, dim=1)\n",
        "          correct += torch.sum(pred==target_).item()\n",
        "          total += target_.size(0)\n",
        "          if (batch_idx) % 20 == 0:\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                    .format(epoch, n_epochs, batch_idx, total_step, loss.item()), file = logfile)\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                    .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
        "      train_acc.append(100 * correct / total)\n",
        "      train_loss.append(running_loss/total_step)\n",
        "      print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}', file = logfile)\n",
        "      print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "      batch_loss = 0\n",
        "      total_t=0\n",
        "      correct_t=0\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          net.eval()\n",
        "          cls.eval()\n",
        "          for data_t, target_t in (test_set):\n",
        "              data_t, target_t = data_t.to(device), target_t.to(device)\n",
        "              outputs_tV = net(data_t)\n",
        "              outputs_t = cls(outputs_tV)\n",
        "              #outputs_t, probs_t = cls(outputs_t1)\n",
        "              loss_t = criterion(outputs_t, target_t)\n",
        "              batch_loss += loss_t.item()\n",
        "              _,pred_t = torch.max(outputs_t, dim=1)\n",
        "              correct_t += torch.sum(pred_t==target_t).item()\n",
        "              total_t += target_t.size(0)\n",
        "          test_acc.append(100 * correct_t/total_t)\n",
        "          test_loss.append(batch_loss/len(test_loader))\n",
        "          network_learned = batch_loss < test_loss_min\n",
        "          print(f'test loss: {np.mean(test_loss):.4f}, test acc: {(100 * correct_t/total_t):.4f}\\n', file = logfile)\n",
        "          print(f'test loss: {np.mean(test_loss):.4f}, test acc: {(100 * correct_t/total_t):.4f}\\n')\n",
        "\n",
        "          if best_test_acc < test_acc[-1]:\n",
        "            #Updating the best test_accuracy obtained\n",
        "            print(\"Best Test Accuracy Updated\", file = logfile)\n",
        "            print(\"Best Test Accuracy Updated\")\n",
        "            best_test_acc = test_acc[-1]\n",
        "          if network_learned:\n",
        "              test_loss_min = batch_loss\n",
        "              torch.save(net.state_dict(), model_save)\n",
        "              torch.save(cls.state_dict(), class_save)\n",
        "              print('Improvement-Detected, save-model to drive', file = logfile)\n",
        "              print('Improvement-Detected, save-model to drive')\n",
        "\n",
        "\n",
        "      net.train()\n",
        "      cls.train()\n",
        "\n",
        "  print(\"The best test accuracy obtained was: \", best_test_acc)\n",
        "  #End of training"
      ]
    }
  ]
}